// model.1.get_ids.js (Fixed: Robust Wait Strategy)
// Fixes "No IDs found" by waiting for valid Regex matches, not just generic links.

'use strict';

const puppeteer = require('puppeteer-extra');
const StealthPlugin = require('puppeteer-extra-plugin-stealth');
const fs = require('fs');
const fsp = fs.promises;
const path = require('path');

puppeteer.use(StealthPlugin());

// --- Configuration ---
const TOP_PLAYER_PAGES = Number(process.env.TOTAL_PAGES || 334);
const BATCH_SIZE       = Number(process.env.BATCH_SIZE   || 25);
const CONCURRENCY      = Math.max(1, Number(process.env.CONCURRENCY || 5));
const MAX_RETRIES      = Math.max(1, Number(process.env.MAX_RETRIES  || 5)); // Increased retries slightly
const HEADLESS         = process.env.HEADLESS === 'false' ? false : true; 

const BASE_URL = 'https://www.fut.gg/players/?page=';
// The ID regex we validated: 26-{id} at the end of the string
const ID_REGEX = /26-(\d+)\/?$/;

const DATA_DIR   = path.resolve(__dirname, '..', 'data');
const TEMP_DIR   = path.join(DATA_DIR, 'raw', 'temp_ids');
const FINAL_FILE = path.join(DATA_DIR, 'player_ids.json');

// TIMEOUTS
const NAV_TIMEOUT_MS = 60_000;
const WAIT_READY_MS  = 30_000;

// Polite delay
const POLITE_DELAY_MIN_MS = 500;
const POLITE_DELAY_MAX_MS = 1500;

// --- Helpers ---
const sleep = (ms) => new Promise(res => setTimeout(res, ms));
const randInt = (min, max) => Math.floor(Math.random() * (max - min + 1)) + min;
const backoff = (attempt) => (3_000 * Math.pow(2, attempt - 1)) + randInt(0, 1_000);

const ensureDir = async (dir) => {
  if (!fs.existsSync(dir)) await fsp.mkdir(dir, { recursive: true });
};

const chunk = (arr, size) => {
  const out = [];
  for (let i = 0; i < arr.length; i += size) out.push(arr.slice(i, i + size));
  return out;
};

const tempPathFor = (i) => path.join(TEMP_DIR, `ids_page_${i}.json`);

const writeJsonAtomic = async (filePath, obj) => {
  const tmp = `${filePath}.tmp`;
  await fsp.writeFile(tmp, JSON.stringify(obj, null, 2), 'utf8');
  await fsp.rename(tmp, filePath);
};

// --- Browser Setup ---
async function createIsolatedContext(browser) {
  if (typeof browser.createBrowserContext === 'function') return await browser.createBrowserContext();
  if (typeof browser.createIncognitoBrowserContext === 'function') return await browser.createIncognitoBrowserContext();
  return browser.defaultBrowserContext();
}

async function newConfiguredPage(context) {
  const page = await context.newPage();
  await page.setViewport({ width: 1366, height: 768 });
  
  // Aggressive blocking for speed
  await page.setRequestInterception(true);
  page.on('request', req => {
    const type = req.resourceType();
    if (['image', 'media', 'font', 'stylesheet', 'other'].includes(type)) req.abort();
    else req.continue();
  });
  return page;
}

// --- Core Scraper ---
async function fetchOnePage(i, context) {
  const url = `${BASE_URL}${i}`;
  const outFile = tempPathFor(i);

  for (let attempt = 1; attempt <= MAX_RETRIES; attempt++) {
    let page = null;
    try {
      page = await newConfiguredPage(context);
      
      // 1. Navigate
      await page.goto(url, { timeout: NAV_TIMEOUT_MS, waitUntil: 'domcontentloaded' });
      
      // 2. THE FIX: Wait for DATA, not just selectors.
      // This waits until the page has at least 5 links that match our specific ID pattern.
      // It completely ignores the header "Players" link.
      await page.waitForFunction((regexStr) => {
          const re = new RegExp(regexStr);
          const anchors = Array.from(document.querySelectorAll('a[href*="/players/"]'));
          // Count how many links match the "26-{id}" pattern
          const validMatches = anchors.filter(a => re.test(a.href)).length;
          // Only return true if we found a reasonable number (e.g., > 5) implies grid loaded
          return validMatches > 5; 
      }, { timeout: WAIT_READY_MS }, ID_REGEX.source);

      // 3. Extract (Now guaranteed to have data)
      const extractedIds = await page.evaluate((regexStr) => {
        const regex = new RegExp(regexStr); 
        const links = Array.from(document.querySelectorAll('a[href*="/players/"]'));
        
        return links
          .map(el => {
            const match = el.getAttribute('href').match(regex);
            return match ? parseInt(match[1], 10) : null;
          })
          .filter(id => id !== null);
      }, ID_REGEX.source);

      if (!extractedIds || extractedIds.length === 0) {
        throw new Error('Logic error: waitForFunction passed but extraction failed.');
      }
      
      const uniqueLocal = [...new Set(extractedIds)];
      
      await writeJsonAtomic(outFile, uniqueLocal);
      console.log(`‚úÖ [p${i}] Got ${uniqueLocal.length} IDs`);
      return; 

    } catch (err) {
      console.warn(`‚ö†Ô∏è [p${i}] Attempt ${attempt} failed: ${err.message}`);
      if (attempt < MAX_RETRIES) await sleep(backoff(attempt));
    } finally {
      if (page) await page.close().catch(() => {});
    }
  }
  console.error(`‚ùå [p${i}] Failed after ${MAX_RETRIES} attempts.`);
}

// --- Worker & Orchestration ---
async function workerLoop(tasks, browser) {
  const context = await createIsolatedContext(browser);
  try {
    while (true) {
      const i = tasks.getNext();
      if (i === null) break;
      if (!fs.existsSync(tempPathFor(i))) {
        await fetchOnePage(i, context);
        await sleep(randInt(POLITE_DELAY_MIN_MS, POLITE_DELAY_MAX_MS));
      }
    }
  } finally {
    if (context.close) await context.close().catch(() => {});
  }
}

function makeTaskDispenser(items) {
  let idx = 0;
  return { getNext: () => (idx < items.length ? items[idx++] : null) };
}

// --- Final Merger ---
async function mergeResults() {
  console.log('\nüîó Merging temporary files...');
  const allIds = new Set();
  
  // Ensure the directory exists before reading
  if (!fs.existsSync(TEMP_DIR)) {
      console.log("‚ö†Ô∏è Temp directory not found (no new pages fetched).");
      return;
  }

  const files = fs.readdirSync(TEMP_DIR).filter(f => f.startsWith('ids_page_') && f.endsWith('.json'));

  if (files.length === 0) {
    console.log('‚ÑπÔ∏è No new data files found to merge.');
    return;
  }

  for (const file of files) {
    try {
      const data = JSON.parse(fs.readFileSync(path.join(TEMP_DIR, file)));
      data.forEach(id => allIds.add(id));
    } catch (e) {
      console.warn(`‚ö†Ô∏è Corrupt file skipped: ${file}`);
    }
  }

  // Load existing IDs if we want to append (Optional, but safer)
  if (fs.existsSync(FINAL_FILE)) {
      try {
          const existing = JSON.parse(fs.readFileSync(FINAL_FILE));
          existing.forEach(id => allIds.add(id));
      } catch(e) {}
  }

  const sortedIds = Array.from(allIds).sort((a, b) => a - b);
  await fsp.writeFile(FINAL_FILE, JSON.stringify(sortedIds, null, 2));
  
  console.log(`\nüéâ SUCCESS: Total unique IDs: ${sortedIds.length}`);
  console.log(`üìÇ Saved to: ${FINAL_FILE}`);
}

// --- Main ---
(async () => {
  console.log("üöÄ Starting Unified ID Scraper (Fixed Wait Strategy)...");
  await ensureDir(TEMP_DIR);

  const allPages = Array.from({ length: TOP_PLAYER_PAGES }, (_, k) => k + 1);
  const batches = chunk(allPages, BATCH_SIZE);

  let browser = null;

  try {
    for (let b = 0; b < batches.length; b++) {
      const batch = batches[b];
      const todo = batch.filter(i => !fs.existsSync(tempPathFor(i)));
      
      if (todo.length === 0) continue;

      if (browser) await browser.close().catch(() => {});
      browser = await puppeteer.launch({ 
          headless: HEADLESS, 
          defaultViewport: null,
          args: ['--no-sandbox', '--disable-setuid-sandbox'] // Helper for stability
      });

      console.log(`\nüì¶ Batch ${b + 1}/${batches.length} (${todo.length} pages)`);
      
      const dispenser = makeTaskDispenser(todo);
      const workers = Array(Math.min(CONCURRENCY, todo.length))
        .fill(null)
        .map(() => workerLoop(dispenser, browser));
      
      await Promise.all(workers);
    }
  } finally {
    if (browser) await browser.close().catch(() => {});
  }

  await mergeResults();
})();